apiVersion: batch/v1
kind: Job
metadata:
  name: training-job
  namespace: train
spec:
  template:
    spec:
      containers:
        - image: train:v1
          imagePullPolicy: IfNotPresent
          name: training1
          command: ["python", "train.py"]
          volumeMounts:
            # cache for torch's datasets
            - name: cachevol
              mountPath: /root/.cache/
            # volume for shared memory, standard fix
            # to support multiple pythorch workers
            - mountPath: /dev/shm
              name: dshm
          env:
            # tell the container where we mounted the cache
            - name: CACHE_PATH
              value: /root/.cache/
          resources:
            limits:
              nvidia.com/gpu: 1 # requesting 1 GPU
      restartPolicy: Never
      volumes:
        - name: cachevol
          persistentVolumeClaim:
            claimName: cachevol-claim
        - name: dshm
          emptyDir:
            medium: Memory
  backoffLimit: 1
